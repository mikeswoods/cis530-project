%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CIS530 final report - Michael Woods and Stuart Wagner
%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (10/6/14)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%-------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%-------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}
\usepackage{fancyvrb}
\input{structure.tex} 

\hyphenation{Fortran hy-phen-ation}

%-------------------------------------------------------------------------------
% TITLE AND AUTHOR(S)
%-------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{CIS530 Final Report}}

\author{\spacedlowsmallcaps{Stuart Wagner \& Michael Woods}}

\date{}

\begin{document}

%-------------------------------------------------------------------------------
% HEADERS
%-------------------------------------------------------------------------------

% The header for all pages (oneside) or for even pages (twoside)
\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}}

% Uncomment when using the twoside option - this modifies the header on
% odd pages
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} 
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}}

 % Enable the headers specified in this block
\pagestyle{scrheadings}

%-------------------------------------------------------------------------------

\maketitle

\setcounter{tocdepth}{2}

%\tableofcontents % Print the table of contents

%-------------------------------------------------------------------------------
% Introduction
%-------------------------------------------------------------------------------

\section{Introduction}

Text classification on the basis of high or low information density has until
recently
\footnote{Nenkova \& Yang ``Detecting Information-Dense Texts in Multiple News Domains'' 2014 \newline 
\url{http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8430/8622}} 
been ignored. Building off the past research, we seek to improve accuracy on
classifying sentences with high information density.

Information density has several applications. Text summarization has relied
partially on the use of KL divergence--the measurement of a difference between
one distribution and another. However, this measure fails to account of
information density. Information dense sentences generally better summarize
information, as they more concisely describe information and ideas. Combined
with a summarizer, density classification could provide a more powerful and
precise text summarization.

Nenkova and Yang used a number of features in developing their model. We used
many similar features, however as students new to NLP, we instead thought that
using a variety of learners could provide better results. We therefore
approached the problem as a machine learning problem that leveraged NLP as
feature inputs. Could we learn which features were more important? Could we
leverage a variety of models, using ensemble learning, to improve accuracy? The
answer to those questions, generally speaking, is yes.

\newpage
 
%-------------------------------------------------------------------------------
% Methods and Resources
%-------------------------------------------------------------------------------

\section{Methods and Resources}

We decided to frame the problem of classifying a given article as either 
information dense or sparse as primarilty a machine learning problem. As with
most problems that can be addressed with the methodologies of machine learning,
having some knowledge of the domain--in this case text classification--can 
provide useful insights when selecting and extract features from raw data. With 
the approaches we learned in CIS530 regarding the analysis of a text and
its structural componnts, along with our own intuitions and experimental 
insights, we outline the various features, tooling, and resources we employed 
for the project below.

\subsection{Data Resources}

% === MRC Psycholinguistic Database ===
\paragraph{\textbf{MRC Psycholinguistic Database}}
\hfill \newline \noindent \url{http://websites.psychology.uwa.edu.au/school/MRCDatabase/uwa_mrc.htm}
\hfill \newline \noindent The MRC Psycholinguistic Database is a machine readable
dictionary containing a number of linguistic and psycholinguistic
attributes and annotation for each word \footnote{\url{http://www.psych.rl.ac.uk/}}. 
Specifically, we primarily made use of the 4,923 lexicon included in the MRC 
distribution.

% === Linguistic Inquiry and Word Count (LIWC) ===
\paragraph{\textbf{Linguistic Inquiry and Word Count (LIWC)}}
\hfill \newline \noindent \url{http://www.liwc.net}
\hfill \newline \noindent The Linguistic Inquiry and Word Count (LIWC) is a text
analysis software program designed to calculate the degree to ``which people use 
different categories of words across a wide array of texts, including emails, 
speeches, poems, or transcribed daily speech''
\footnote{\url{http://www.liwc.net/}}.

\subsection{Tools and Software Packages}

% === Stanford CoreNLP ===
\paragraph{\textbf{Stanford CoreNLP}}
\hfill \newline \noindent \url{http://nlp.stanford.edu/software/corenlp.shtml}
\hfill \newline \noindent A suite of text processing tools capable of part-of-
speech (POS) tagging, named entity recognition (NER), tokenization,
lemmatization, sentiment analysis, as well as dependency and syntactic parse
extraction.

% === scikit-learn ===
\paragraph{\textbf{scikit-learn}}
\hfill \newline \noindent \url{scikit-learn http://scikit-learn.org/}
\hfill \newline \noindent A popular Python machine learning framework with
implementations for a large selection of popular classifiers.

% === NumPy ===
\paragraph{\textbf{NumPy}}
\hfill \newline \noindent \url{http://www.numpy.org}
\hfill \newline \noindent A scientific computing package for Python providing
native n-dimensional array datatype, as well as various linear algebra,
statistical, and general mathematical functions suitable for ``''number
crunching''.

% === NLTK ===
\paragraph{\textbf{NLTK}}
\hfill \newline \noindent \url{http://www.nltk.org/} 
\hfill \newline \noindent A popular Python library for working with natural 
language that includes a number of convenient text processing tools and built 
in corpora for multiple languages.

%------------------------------------------------

\subsection{Features}

Building off the work of Nenkova and Yang, we leveraged a number of similar 
features in our approach to the problem of textual information density
classification.

% === CoreNLP word count--binary bag-of-words ===
\paragraph{\textbf{(1) CoreNLP word count--binary bag-of-words}}
\hfill \newline \noindent \textit{9799 dimensions}. All unique tokens occurring
between \texttt{<word></word>} tags in the generated XML obtained from running
all training and test files through the CoreNLP tool were collected into a list
and converted to lowercase. Any tokens consisting of stop words, as defined by
NLTK's English stop word list or punctuation characters were removed. The list
was then sorted alphabetically in ascending order and each word was assigned an
index. To construct the feature vector for a given lead, the text of the lead
was processed in a manner similar to that of the XML output and a vector the
size of the bag-of-words model was initialized with zeros. If the lead contained
the $i$-th word in the bag-of-words model, the $i$-th position in the feature
vector was set to $1$.

% === CoreNLP sentence information ===
\paragraph{\textbf{(2) CoreNLP sentence information}} 
\hfill \newline \noindent \textit{7 dimensions}. To construct the feature
vector, each processed lead had a number of pieces of information extracted from
it directly or from the XML file generated from it using CoreNLP. These
quantities define the features indicated below along with our prediction on itâ€™s
impact on information density ($+$/$-$):
\begin{itemize}
	\item The number of sentences per lead. ($-$)
	\item The number of non-unique tokens per lead. ($-$)
	\item The number of named entities recognized in the lead. ($+$)
	\item The number of nouns in the lead (tokens tagged as \texttt{NN}, \texttt{NNS}, \texttt{NNP}, or \texttt{NNPS}) ($+$)
	\item The number of words with over six letters. ($+$)
	\item The number of quotation marks appearing in the lead. ($+$)
	\item The aggregate absolute sentiment score of the lead. Each sentence was assigned a numeric score according to the following rubric: ``very negative'': $2$, ``negative'': $1$, ``neutral'': $0$, ``positive'': $1$, and ``very positive'': $2$. The scores were then summed and the absolute value was taken. ($-$)
\end{itemize}

% === LIWC ===
\paragraph{\textbf{(3) LIWC}}
\hfill \newline \noindent \textit{78 dimensions}. Our hypothesis was that words
indicating sentiment and emotions might indicate sentences that were less
information dense. We thought these might be positively correlated with
information density: present tense, numbers, impersonal pronouns, and causation.
We also believed multiple attributes may be negatively correlated, including:
1st person usage, auxillary verbs and past tense, quantifiers, and positive and
negative emotion.

% === CoreNLP production rule count--binary bag-of-words ===
\paragraph{\textbf{(4) CoreNLP production rule count--binary bag-of-words}}
\hfill \newline \noindent \textit{5654 dimensions}. The syntactic parse
generated by CoreNLP for each sentence from a given lead is extracted from the
corresponding \texttt{<parse></parse>} tag in the corresponding XML file. A
parse tree is constructed in the same manner as described in Homework 3, where
only non- terminal nodes are considered. To construct the master list of
production rules, the outlined process is repeated for every lead file in the
training and test set and every unique production rule is collected into a list.
Each rule is then assigned an index, $0$ to $N-1$, where N is the number of
unique production rules. Then, to construct a feature vector for a given lead, a
$1 x N$ vector of zeros is created and every production rule occurring in the
sentences of the lead is extracted. For every rule in the master list, if the
$i$-th rule occurs in the rules extracted from the lead, the $i$-th index in the
feature vector is set to $1$. The belief was that perhaps certain production
rules may be related with information density.

% === LIWC ===
\paragraph{\textbf{(5) MRC dictionary--binary bag-of-words}}
\hfill \newline \noindent \textit{4923 dimensions}. 
The MRC Psycholinguistic database word dictionary consisting of $4,923$ words
was used to construct a binary bag-of-words model in a similar manner to the
CoreNLP binary bag-of-words model. We also attempted counts normalized by length
of lead, however there did not appear to be a significant difference in
accuracy. The text of each lead was tokenized, converted to lowercase and
stripped of stop words. The benefit of using this approach is that the
dictionary is independent of the task and training data (Nenkova and Yang).

% === Dependency relations--binary bag-of-words ===
\paragraph{\textbf{(6) Dependency relations--binary bag-of-words}}
\hfill \newline \noindent \textit{5000 dimensions}. 
For each lead file in the training set, the <dep></dep> nodes were extracted 
from the corresponding CoreNLP XML files and converted to a 3-tuple consisting 
of the dependency relation type, the governor word, and the dependent word. 
Each tuple instance was counted, and the top 5000 dependency tuples were used 
to build a bag-of-words vector in much the same manner as features (1), (4)
and (5). Constructing the feature vector for a given lead proceeded in much the
same manner as (5) as well. 

%------------------------------------------------

\subsection{Models}
Regarding the actual task of classification, we constructed several 
classification models using the features described above. The model 
configurations are as follows:

\begin{itemize}
	\item Logistic regression with a L2 penalty and class weights $1$: $0.58$ and
	$-1$: $0.42$ set to reflect the true distribution of labels in the test set.

	\item Linear SVM regression with L2 loss and penalty functions and class 
	weights $1$: $0.58$ and $-1$: $0.42$ set to reflect the true distribution of
	labels in the test set.

	\item Bernoulli naive bayes with default parameters.

	\item Ensemble method with the three classifiers described above. Each 
	classifier makes a prediction for a given observation and a majority vote is
	taken to determine the final predicted label.
\end{itemize}

%-------------------------------------------------------------------------------
% Final system
%-------------------------------------------------------------------------------
\break
\section{Final system}

\subsection{Team}

We are team ``$W^2$'', or as it appears on the leaderboard in ASCII,
\texttt{W\string^2} in.

\subsection{System Design}

From the very onset of the project, we exercised great discipline in producing
what we believe to a clean, extensible system for making predictions about text.
The software could easily have been treated as disposable--and designed as such
--since the predictions produced by the system are ultimately the real value and
not the software itself. Our rationale for putting a large, up-front effort into
properly designing a data pipeline was to introduce a lower mental overhead cost
when quickly iterating various featuresets and models as the project progressed.
The effort paid of many times over, as creating new models and features was the
same as defining a new Python module. The model or feature could then later be
referred to by name and maniupulated through set of generic functions.

\subsection{Models}

All models are implemented as regular Python modules in the
\texttt{project.models} package in the project \texttt{code} directory.
Additionally, all model modules are expected to define the functions described
below, as this provides a common interface to interact with the model
implementation: 
\begin{Verbatim}[frame=single]
def preprocess(model_name, train_files, test_files)
def train(model_name, train_files,  labels, *args, **kwargs)
def predict(model_name, model_obj, test_files, *args, **kwargs)
\end{Verbatim}

\subsubsection{Lookup}
\noindent A module can then easily be looked up by name like so:
\begin{Verbatim}[frame=single]
linear_model = get_model('linear_model')
\end{Verbatim}
\noindent which is defined in the following simple function which makes use
of the \texttt{importlib} from the Python standard libary:
\begin{Verbatim}[frame=single]
def get_model(name):
    return importlib.import_module('project.models.'+name)
\end{Verbatim}

\subsection{Features}

\noindent Much like models, features are also implemented as Python modules, as
this allows for quick prototyping and iterative refirement of new feature types.

\noindent In a similar manner to models described above, features (or more
accurately feature-izers) can easily be resolved with the following function:

\begin{Verbatim}[frame=single]
def get_feature(name): return
    importlib.import_module('project.features.'+name)
\end{Verbatim}

\noindent Features can also expose the following functions

\begin{Verbatim}[frame=single]
def build(feature_name, *args, **kwargs)
def featurize(feature_name, feature_obj=None, *args, **kwargs)
\end{Verbatim}

\noindent where \texttt{build} performs the initial setup work needed to
featurize an input, and \texttt{featurize} performs the actual conversion of an
input into a feature vector.

\subsection{Pipeline}

The conceptual classification pipeline is outlined below in pseudocode, which
describes the general flow of information in the system, initially from raw
input text to final predicted labels:

\begin{Verbatim}[frame=single]
TRAIN_FILES = framework.get_train_files()

if in-submission-mode:
    TEST_FILES = framework.get_test_files()
else:
    TEST_FILES = TRAIN_FILES

# if the model defines an optional preprocess() function, 
# call it:
if is_preprocess_defined('foo')
    optional = preprocess('foo', TRAIN_FILES, TEST_FILES)
else:
    optional = []

model_obj = train('foo', TRAIN_FILES, TRAIN_LABELS, *optional)

return predict('foo', model_obj, TEST_FILES, *optional)
\end{Verbatim}

\subsection{Testing}

Finally, the above pipeline is invoked from within the context of a testing
framework we designed. The framework itself provides the input files to the 
prediction pipeline, which may vary depending on whether an actual prediction
submission is being generated. If a leaderboard submission is to be generated, 
then actual test data is supplied to the pipeline by the framework, otherwise 
some holdout from the training data is used as test data. After the predictions 
have been made, the framework provides basic reporting information regarding 
the accuracy, precision, and F-score of the results.

%-------------------------------------------------------------------------------
% Outcomes
%-------------------------------------------------------------------------------
\break
\section{Outcomes}

All experiments were conducted using the \texttt{scikit-learn} 
\texttt{cross\_validation.train\_test\_split()}, function with $10\%$ of
the training data held out as the test set. \\

\noindent The all submission configurations and related results are 
summarized in the table below. All feature sets are referenced by number as 
given in section 2.3.

\subsection{Submission results}
\begin{center}
    \begin{tabular}{| c | c | l | l|}
    \hline
    Attempt & Accuracy & Model               & Feature sets    \\ \hline
    1       & $75.6\%$ & Logistic regression  & $1$, $2$, $3$  \\ \hline
    2       & $76.0\%$ & Logistic regression /w PCA(n=500)+gridsearch &  $2$, $3$, $4$, $5$ \\ \hline
    3       & $78.4\%$ & Ensemble classifier (LR+SVM+NB) &$2$, $3$, $4$, $5$       \\ \hline
    4       & $78.4\%$ & Ensemble classifier (LR+SVM+NB) & $1$, $2$, $3$, $4$, $5$ \\ \hline
    5       & $79.6\%$ & Ensemble classifier (LR+SVM+NB) & $2$, $3$, $4$, $5$, $6$ \\ \hline
    \end{tabular}
\end{center}

%-------------------------------------------------------------------------------
% Discussion of Results and Conclusion
%-------------------------------------------------------------------------------
\section{Discussion of Results and Conclusion}

Overall, we are quite pleased with our results. Each test submission either 
resulted in a higher accuracy, or at the very least the same score. We believe
this is due to the design of our system which allowed for quick a 
test--prototype--report cycle of various model and feature configurations




\end{document}
